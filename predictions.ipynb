{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'xgboost'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_23768\\3027683070.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mxgboost\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mxgb\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodel_selection\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mtrain_test_split\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mGridSearchCV\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmetrics\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mmake_scorer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mroc_auc_score\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'xgboost'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import xgboost as xgb\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.metrics import make_scorer, roc_auc_score\n",
    "from sklearn.ensemble import StackingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.base import BaseEstimator, ClassifierMixin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DOUBLE11_DAY = 184\n",
    "\n",
    "DIR_1 = 'data_format1/'\n",
    "DIR_2 = 'data_format2/'\n",
    "\n",
    "PATH_TRAIN = DIR_1 + 'train_format1.csv'\n",
    "PATH_TEST = DIR_1 + 'test_format1.csv'\n",
    "PATH_USER_INFO = DIR_1 + 'user_info_format1.csv'\n",
    "PATH_USER_LOG = DIR_1 + 'user_log_format1.csv'\n",
    "\n",
    "df_train = pd.read_csv(PATH_TRAIN)    \n",
    "df_test = pd.read_csv(PATH_TEST) # this data has nan for the probabilities, the task is to predict the nan values. \n",
    "df_user_info = pd.read_csv(PATH_USER_INFO)\n",
    "df_user_log = pd.read_csv(PATH_USER_LOG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic Data Preprocessing\n",
    "df_user_log.rename(columns={'seller_id': 'merchant_id'}, inplace=True)\n",
    "\n",
    "df_user_log['brand_id'].fillna(0, inplace=True)\n",
    "df_user_log['time_stamp'] = (pd.to_datetime(df_user_log['time_stamp'], format='%m%d') - pd.to_datetime(df_user_log['time_stamp'].min(), format='%m%d')).dt.days\n",
    "\n",
    "# Optimize memory usage by converting data types\n",
    "df_user_log['user_id'] = df_user_log['user_id'].astype('int32')\n",
    "df_user_log['item_id'] = df_user_log['item_id'].astype('int32')\n",
    "df_user_log['cat_id'] = df_user_log['cat_id'].astype('int16')\n",
    "df_user_log['merchant_id'] = df_user_log['merchant_id'].astype('int16')\n",
    "df_user_log['brand_id'] = df_user_log['brand_id'].astype('int16')\n",
    "df_user_log['time_stamp'] = df_user_log['time_stamp'].astype('int16')\n",
    "df_user_log['action_type'] = df_user_log['action_type'].astype('int8')\n",
    "\n",
    "df_user_info['gender'].fillna(2, inplace=True)  # 0 for female, 1 for male, 2 and NULL for unknown.\n",
    "df_user_info['gender'] = df_user_info['gender'].astype('int8')\n",
    "df_user_info['age_range'].fillna(0, inplace=True)  # 1 for <18; 2 for [18,24]; 3 for [25,29]; 4 for [30,34]; 5 for [35,39]; 6 for [40,49]; 7 and 8 for >= 50; 0 and NULL for unknown.\n",
    "df_user_info['age_range'] = df_user_info['age_range'].astype('int8')\n",
    "df_user_info['user_id'] = df_user_info['user_id'].astype('int32')\n",
    "\n",
    "# Feature Engineering\n",
    "df_user_log['time_month'] = df_user_log['time_stamp'] // 30\n",
    "df_user_log['time_week'] = df_user_log['time_stamp'] // 7\n",
    "\n",
    "# Grouping merchants, users, and user-merchant pairs\n",
    "merchants = df_user_log.groupby('merchant_id')\n",
    "users = df_user_log.groupby('user_id')\n",
    "merchants_users = df_user_log.groupby(['merchant_id', 'user_id'])\n",
    "\n",
    "# Features: Merchant and User Statistics\n",
    "to_merge = merchants.nunique().reset_index().rename(columns={'item_id': 'items_merchant', 'cat_id': 'categories_merchant', 'user_id': 'users_merchant', 'brand_id': 'brands_merchant'})\n",
    "df_train = df_train.merge(to_merge, on='merchant_id', how='left')\n",
    "\n",
    "to_merge = users.nunique().reset_index().rename(columns={'item_id': 'items_user', 'cat_id': 'categories_user', 'merchant_id': 'merchants_user', 'brand_id': 'brands_user'})\n",
    "df_train = df_train.merge(to_merge, on='user_id', how='left')\n",
    "\n",
    "to_merge = merchants_users.nunique().reset_index().rename(columns={'item_id': 'items_user_merchant', 'cat_id': 'categories_user_merchant', 'brand_id': 'brands_user_merchant'})\n",
    "df_train = df_train.merge(to_merge, on=['user_id', 'merchant_id'], how='left')\n",
    "\n",
    "# Adding action type features\n",
    "to_merge = users['action_type'].value_counts().unstack(fill_value=0).rename(columns={0: 'clicks_user', 1: 'add_to_carts_user', 2: 'purchases_user', 3: 'add_to_favorites_user'})\n",
    "df_train = df_train.merge(to_merge, on='user_id', how='left')\n",
    "\n",
    "to_merge = merchants['action_type'].value_counts().unstack(fill_value=0).rename(columns={0: 'clicks_merchant', 1: 'add_to_carts_merchant', 2: 'purchases_merchant', 3: 'add_to_favorites_merchant'})\n",
    "df_train = df_train.merge(to_merge, on='merchant_id', how='left')\n",
    "\n",
    "to_merge = merchants_users['action_type'].value_counts().unstack(fill_value=0).rename(columns={0: 'clicks_user_merchant', 1: 'add_to_carts_user_merchant', 2: 'purchases_user_merchant', 3: 'add_to_favorites_user_merchant'})\n",
    "df_train = df_train.merge(to_merge, on=['user_id', 'merchant_id'], how='left')\n",
    "\n",
    "# Ratio Features\n",
    "EPS = 1e-8\n",
    "df_train['clicks_in_user_ratio'] = df_train['clicks_user_merchant'] / (df_train['clicks_merchant'] + EPS)\n",
    "df_train['carts_in_user_ratio'] = df_train['add_to_carts_user_merchant'] / (df_train['add_to_carts_merchant'] + EPS)\n",
    "df_train['purchases_in_user_ratio'] = df_train['purchases_user_merchant'] / (df_train['purchases_merchant'] + EPS)\n",
    "df_train['favourites_in_user_ratio'] = df_train['add_to_favorites_user_merchant'] / (df_train['add_to_favorites_merchant'] + EPS)\n",
    "\n",
    "df_train['clicks_in_merchant_ratio'] = df_train['clicks_user_merchant'] / (df_train['clicks_user'] + EPS)\n",
    "df_train['carts_in_merchant_ratio'] = df_train['add_to_carts_user_merchant'] / (df_train['add_to_carts_user'] + EPS)\n",
    "df_train['purchases_in_merchant_ratio'] = df_train['purchases_user_merchant'] / (df_train['purchases_user'] + EPS)\n",
    "df_train['favourites_in_merchant_ratio'] = df_train['add_to_favorites_user_merchant'] / (df_train['add_to_favorites_user'] + EPS)\n",
    "\n",
    "df_train['temporary_total_actions_merchant'] = (df_train['clicks_merchant'] + df_train['add_to_carts_merchant'] + df_train['purchases_merchant'] + df_train['add_to_favorites_merchant'] + EPS)\n",
    "df_train['clicks_ratio_merchant'] = df_train['clicks_merchant'] / (df_train['temporary_total_actions_merchant'])\n",
    "df_train['carts_ratio_merchant'] = df_train['add_to_carts_merchant'] / (df_train['temporary_total_actions_merchant'])\n",
    "df_train['purchases_ratio_merchant'] = df_train['purchases_merchant'] / (df_train['temporary_total_actions_merchant'])\n",
    "df_train['favourites_ratio_merchant'] = df_train['add_to_favorites_merchant'] / (df_train['temporary_total_actions_merchant'])\n",
    "df_train.drop('temporary_total_actions_merchant', axis=1, inplace=True)\n",
    "\n",
    "df_train['temporary_total_actions_user'] = (df_train['clicks_user'] + df_train['add_to_carts_user'] + df_train['purchases_user'] + df_train['add_to_favorites_user'] + EPS)\n",
    "df_train['clicks_ratio_user'] = df_train['clicks_user'] / (df_train['temporary_total_actions_user'])\n",
    "df_train['carts_ratio_user'] = df_train['add_to_carts_user'] / (df_train['temporary_total_actions_user'])\n",
    "df_train['purchases_ratio_user'] = df_train['purchases_user'] / (df_train['temporary_total_actions_user'])\n",
    "df_train['favourites_ratio_user'] = df_train['add_to_favorites_user'] / (df_train['temporary_total_actions_user'])\n",
    "df_train.drop('temporary_total_actions_user', axis=1, inplace=True)\n",
    "\n",
    "df_train['temporary_total_actions_user_merchant'] = (df_train['clicks_user_merchant'] + df_train['add_to_carts_user_merchant'] + df_train['purchases_user_merchant'] + df_train['add_to_favorites_user_merchant'] + EPS)\n",
    "df_train['clicks_ratio_user_merchant'] = df_train['clicks_user_merchant'] / (df_train['temporary_total_actions_user_merchant'])\n",
    "df_train['carts_ratio_user_merchant'] = df_train['add_to_carts_user_merchant'] / (df_train['temporary_total_actions_user_merchant'])\n",
    "df_train['purchases_ratio_user_merchant'] = df_train['purchases_user_merchant'] / (df_train['temporary_total_actions_user_merchant'])\n",
    "df_train['favourites_ratio_user_merchant'] = df_train['add_to_favorites_user_merchant'] / (df_train['temporary_total_actions_user_merchant'])\n",
    "df_train.drop('temporary_total_actions_user_merchant', axis=1, inplace=True)\n",
    "\n",
    "\n",
    "\n",
    "# # # 1. 读取和清理测试集\n",
    "# # df_test = pd.read_csv(PATH_TEST)\n",
    "# #\n",
    "# # # 对测试集进行相同的预处理步骤\n",
    "# # df_user_log = df_test.copy()\n",
    "#\n",
    "# # 将\"seller_id\"列重命名为\"merchant_id\"\n",
    "# df_user_log.rename(columns={'seller_id': 'merchant_id'}, inplace=True)\n",
    "#\n",
    "# # 填充缺失值\n",
    "# df_user_log['brand_id'].fillna(0, inplace=True)\n",
    "# df_user_log['time_stamp'] = (pd.to_datetime(df_user_log['time_stamp'], format='%m%d') - pd.to_datetime(df_user_log['time_stamp'].min(), format='%m%d')).dt.days\n",
    "#\n",
    "# # 优化内存\n",
    "# df_user_log['user_id'] = df_user_log['user_id'].astype('int32')\n",
    "# df_user_log['item_id'] = df_user_log['item_id'].astype('int32')\n",
    "# df_user_log['cat_id'] = df_user_log['cat_id'].astype('int16')\n",
    "# df_user_log['merchant_id'] = df_user_log['merchant_id'].astype('int16')\n",
    "# df_user_log['brand_id'] = df_user_log['brand_id'].astype('int16')\n",
    "# df_user_log['time_stamp'] = df_user_log['time_stamp'].astype('int16')\n",
    "# df_user_log['action_type'] = df_user_log['action_type'].astype('int8')\n",
    "#\n",
    "# # 2. 对df_user_info做相同的预处理\n",
    "# df_user_info = df_user_info.copy()\n",
    "#\n",
    "# df_user_info['gender'].fillna(2, inplace=True)  # 0 for female, 1 for male, 2 and NULL for unknown.\n",
    "# df_user_info['gender'] = df_user_info['gender'].astype('int8')\n",
    "# df_user_info['age_range'].fillna(0, inplace=True)  # 1 for <18; 2 for [18,24]; 3 for [25,29]; 4 for [30,34]; 5 for [35,39]; 6 for [40,49]; 7 and 8 for >= 50; 0 and NULL for unknown.\n",
    "# df_user_info['age_range'] = df_user_info['age_range'].astype('int8')\n",
    "# df_user_info['user_id'] = df_user_info['user_id'].astype('int32')\n",
    "#\n",
    "# # 3. 添加时间特征（与训练集相同的操作）\n",
    "# df_user_log['time_month'] = df_user_log['time_stamp'] // 30\n",
    "# df_user_log['time_week'] = df_user_log['time_stamp'] // 7\n",
    "#\n",
    "# # 4. 计算商户、用户以及商户用户对的统计特征\n",
    "# merchants = df_user_log.groupby('merchant_id')\n",
    "# users = df_user_log.groupby('user_id')\n",
    "# merchants_users = df_user_log.groupby(['merchant_id', 'user_id'])\n",
    "\n",
    "# 商户统计特征\n",
    "to_merge = merchants.nunique().reset_index().rename(columns={'item_id': 'items_merchant', 'cat_id': 'categories_merchant', 'user_id': 'users_merchant', 'brand_id': 'brands_merchant'})\n",
    "df_test = df_test.merge(to_merge, on='merchant_id', how='left')\n",
    "\n",
    "# 用户统计特征\n",
    "to_merge = users.nunique().reset_index().rename(columns={'item_id': 'items_user', 'cat_id': 'categories_user', 'merchant_id': 'merchants_user', 'brand_id': 'brands_user'})\n",
    "df_test = df_test.merge(to_merge, on='user_id', how='left')\n",
    "\n",
    "# 商户用户对统计特征\n",
    "to_merge = merchants_users.nunique().reset_index().rename(columns={'item_id': 'items_user_merchant', 'cat_id': 'categories_user_merchant', 'brand_id': 'brands_user_merchant'})\n",
    "df_test = df_test.merge(to_merge, on=['user_id', 'merchant_id'], how='left')\n",
    "\n",
    "# 5. 添加操作类型特征\n",
    "# 用户的操作类型统计\n",
    "to_merge = users['action_type'].value_counts().unstack(fill_value=0).rename(columns={0: 'clicks_user', 1: 'add_to_carts_user', 2: 'purchases_user', 3: 'add_to_favorites_user'})\n",
    "df_test = df_test.merge(to_merge, on='user_id', how='left')\n",
    "\n",
    "# 商户的操作类型统计\n",
    "to_merge = merchants['action_type'].value_counts().unstack(fill_value=0).rename(columns={0: 'clicks_merchant', 1: 'add_to_carts_merchant', 2: 'purchases_merchant', 3: 'add_to_favorites_merchant'})\n",
    "df_test = df_test.merge(to_merge, on='merchant_id', how='left')\n",
    "\n",
    "# 商户用户对的操作类型统计\n",
    "to_merge = merchants_users['action_type'].value_counts().unstack(fill_value=0).rename(columns={0: 'clicks_user_merchant', 1: 'add_to_carts_user_merchant', 2: 'purchases_user_merchant', 3: 'add_to_favorites_user_merchant'})\n",
    "df_test = df_test.merge(to_merge, on=['user_id', 'merchant_id'], how='left')\n",
    "\n",
    "# 6. 添加比例特征\n",
    "EPS = 1e-8\n",
    "df_test['clicks_in_user_ratio'] = df_test['clicks_user_merchant'] / (df_test['clicks_merchant'] + EPS)\n",
    "df_test['carts_in_user_ratio'] = df_test['add_to_carts_user_merchant'] / (df_test['add_to_carts_merchant'] + EPS)\n",
    "df_test['purchases_in_user_ratio'] = df_test['purchases_user_merchant'] / (df_test['purchases_merchant'] + EPS)\n",
    "df_test['favourites_in_user_ratio'] = df_test['add_to_favorites_user_merchant'] / (df_test['add_to_favorites_merchant'] + EPS)\n",
    "\n",
    "df_test['clicks_in_merchant_ratio'] = df_test['clicks_user_merchant'] / (df_test['clicks_user'] + EPS)\n",
    "df_test['carts_in_merchant_ratio'] = df_test['add_to_carts_user_merchant'] / (df_test['add_to_carts_user'] + EPS)\n",
    "df_test['purchases_in_merchant_ratio'] = df_test['purchases_user_merchant'] / (df_test['purchases_user'] + EPS)\n",
    "df_test['favourites_in_merchant_ratio'] = df_test['add_to_favorites_user_merchant'] / (df_test['add_to_favorites_user'] + EPS)\n",
    "\n",
    "df_test['temporary_total_actions_merchant'] = (df_test['clicks_merchant'] + df_test['add_to_carts_merchant'] + df_test['purchases_merchant'] + df_test['add_to_favorites_merchant'] + EPS)\n",
    "df_test['clicks_ratio_merchant'] = df_test['clicks_merchant'] / (df_test['temporary_total_actions_merchant'])\n",
    "df_test['carts_ratio_merchant'] = df_test['add_to_carts_merchant'] / (df_test['temporary_total_actions_merchant'])\n",
    "df_test['purchases_ratio_merchant'] = df_test['purchases_merchant'] / (df_test['temporary_total_actions_merchant'])\n",
    "df_test['favourites_ratio_merchant'] = df_test['add_to_favorites_merchant'] / (df_test['temporary_total_actions_merchant'])\n",
    "df_test.drop('temporary_total_actions_merchant', axis=1, inplace=True)\n",
    "\n",
    "df_test['temporary_total_actions_user'] = (df_test['clicks_user'] + df_test['add_to_carts_user'] + df_test['purchases_user'] + df_test['add_to_favorites_user'] + EPS)\n",
    "df_test['clicks_ratio_user'] = df_test['clicks_user'] / (df_test['temporary_total_actions_user'])\n",
    "df_test['carts_ratio_user'] = df_test['add_to_carts_user'] / (df_test['temporary_total_actions_user'])\n",
    "df_test['purchases_ratio_user'] = df_test['purchases_user'] / (df_test['temporary_total_actions_user'])\n",
    "df_test['favourites_ratio_user'] = df_test['add_to_favorites_user'] / (df_test['temporary_total_actions_user'])\n",
    "df_test.drop('temporary_total_actions_user', axis=1, inplace=True)\n",
    "\n",
    "df_test['temporary_total_actions_user_merchant'] = (df_test['clicks_user_merchant'] + df_test['add_to_carts_user_merchant'] + df_test['purchases_user_merchant'] + df_test['add_to_favorites_user_merchant'] + EPS)\n",
    "df_test['clicks_ratio_user_merchant'] = df_test['clicks_user_merchant'] / (df_test['temporary_total_actions_user_merchant'])\n",
    "df_test['carts_ratio_user_merchant'] = df_test['add_to_carts_user_merchant'] / (df_test['temporary_total_actions_user_merchant'])\n",
    "df_test['purchases_ratio_user_merchant'] = df_test['purchases_user_merchant'] / (df_test['temporary_total_actions_user_merchant'])\n",
    "df_test['favourites_ratio_user_merchant'] = df_test['add_to_favorites_user_merchant'] / (df_test['temporary_total_actions_user_merchant'])\n",
    "df_test.drop('temporary_total_actions_user_merchant', axis=1, inplace=True)\n",
    "\n",
    "# # 7. 保留特征并预测 'probability' (将probability列剔除)\n",
    "# X_test = df_test.drop(columns=['probability'])\n",
    "\n",
    "# Now you can use the model to predict the 'probability' column on X_test\n",
    "\n",
    "# 在开始训练之前添加提醒\n",
    "\n",
    "print(\"开始训练过程...\")\n",
    "# # Splitting into train and test\n",
    "# X = df_train.drop(['probability'], axis=1)\n",
    "# y = df_train['probability']\n",
    "#\n",
    "# X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "#\n",
    "# # Define the models\n",
    "# # lgbm = lgb.LGBMClassifier(n_estimators=100)\n",
    "# xgb_model = xgb.XGBClassifier(n_estimators=10, tree_method='gpu_hist', gpu_id=0)\n",
    "# catboost_model = cb.CatBoostClassifier(iterations=10, learning_rate=0.05, depth=5, silent=True, task_type='GPU',  train_dir=r\"C:\\Users\\25442\\Desktop\\data_format1\\data_format1\\5\")\n",
    "#\n",
    "# # Define the hyperparameter grids\n",
    "# # lgbm_param_grid = {\n",
    "# #     'n_estimators': [50, 100, 150],\n",
    "# #     'learning_rate': [0.01, 0.05, 0.1],\n",
    "# #     'max_depth': [6, 8, 10],\n",
    "# #     'num_leaves': [31, 50, 70]\n",
    "# # }\n",
    "#\n",
    "# xgb_param_grid = {\n",
    "#     'n_estimators': [50, 100, 150],\n",
    "#     'learning_rate': [0.01, 0.05, 0.1],\n",
    "#     'max_depth': [6, 8, 10],\n",
    "#     'subsample': [0.7, 0.8, 0.9],\n",
    "#     'colsample_bytree': [0.7, 0.8, 0.9]\n",
    "# }\n",
    "#\n",
    "# catboost_param_grid = {\n",
    "#     'iterations': [50, 100, 150],\n",
    "#     'learning_rate': [0.01, 0.05, 0.1],\n",
    "#     'depth': [6, 8, 10],\n",
    "#     'l2_leaf_reg': [3, 5, 7]\n",
    "# }\n",
    "#\n",
    "# # Define the scorer\n",
    "# roc_auc_scorer = make_scorer(roc_auc_score, greater_is_better=True)\n",
    "#\n",
    "# # Perform GridSearchCV for each model\n",
    "# # lgbm_grid_search = GridSearchCV(estimator=lgbm, param_grid=lgbm_param_grid, cv=5, scoring=roc_auc_scorer, n_jobs=-1)\n",
    "# # lgbm_grid_search.fit(X_train, y_train)\n",
    "#\n",
    "# xgb_grid_search = GridSearchCV(estimator=xgb_model, param_grid=xgb_param_grid, cv=5, scoring=roc_auc_scorer, n_jobs=-1)\n",
    "# xgb_grid_search.fit(X_train, y_train)\n",
    "#\n",
    "# catboost_grid_search = GridSearchCV(estimator=catboost_model, param_grid=catboost_param_grid, cv=5, scoring=roc_auc_scorer, n_jobs=-1)\n",
    "# catboost_grid_search.fit(X_train, y_train)\n",
    "#\n",
    "# # Use the best models for stacking\n",
    "# # best_lgbm = lgbm_grid_search.best_estimator_\n",
    "# best_xgb = xgb_grid_search.best_estimator_\n",
    "# best_catboost = catboost_grid_search.best_estimator_\n",
    "#\n",
    "# # Stacking model with tuned models\n",
    "# # stacking_model = StackingClassifier(\n",
    "# #     estimators=[('lgbm', best_lgbm), ('xgb', best_xgb), ('catboost', best_catboost)],\n",
    "# #     final_estimator=LogisticRegression()\n",
    "# # )\n",
    "# stacking_model = StackingClassifier(\n",
    "#     estimators=[('xgb', best_xgb), ('catboost', best_catboost)],\n",
    "#     final_estimator=LogisticRegression()\n",
    "# )\n",
    "# # Fit the stacking model\n",
    "# stacking_model.fit(X_train, y_train)\n",
    "#\n",
    "# # Predict on validation set\n",
    "# y_pred = stacking_model.predict(X_valid)\n",
    "# # 对测试集进行预测\n",
    "# df_test['probability'] = stacking_model.predict(df_test.drop(['probability'], axis=1))\n",
    "#\n",
    "# # 输出结果到csv文件\n",
    "# df_test[['user_id', 'merchant_id', 'probability']].to_csv('predictions.csv', index=False)\n",
    "# Split the data\n",
    "\n",
    "# 分割特征和目标值\n",
    "X = df_train.drop(['probability'], axis=1)  # 将 'probability' 作为目标值，剩余的列作为特征\n",
    "y = df_train['probability']\n",
    "\n",
    "# 拆分训练集和验证集\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=0.5, random_state=42)\n",
    "\n",
    "print(\"训练集和验证集已经成功拆分\")\n",
    "# # Split data into training and validation sets\n",
    "# X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=0.5, random_state=42)\n",
    "\n",
    "# Neural Network Model using Keras\n",
    "print(df_train.columns)\n",
    "\n",
    "def create_nn_model():\n",
    "    model = Sequential()\n",
    "    model.add(Dense(64, input_dim=X_train.shape[1], activation='relu'))\n",
    "    model.add(Dense(32, activation='relu'))\n",
    "    model.add(Dense(1, activation='sigmoid'))  # Output layer for binary classification\n",
    "    model.compile(optimizer=Adam(), loss='binary_crossentropy', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "# Create the neural network model\n",
    "nn_model = create_nn_model()\n",
    "\n",
    "# EarlyStopping callback\n",
    "early_stopping = EarlyStopping(monitor='val_loss',  # Monitor validation loss\n",
    "                               patience=5,  # Number of epochs to wait before stopping\n",
    "                               restore_best_weights=True,  # Restore the best weights after stopping\n",
    "                               verbose=1)\n",
    "\n",
    "# Train the Neural Network with validation data and early stopping\n",
    "nn_model.fit(X_train, y_train, epochs=100, batch_size=32, verbose=1,\n",
    "             validation_data=(X_valid, y_valid),  # Use validation data\n",
    "             callbacks=[early_stopping])  # Add early stopping callback\n",
    "\n",
    "# Define the XGBoost model (remains the same as before)\n",
    "xgb_model = xgb.XGBClassifier(n_estimators=10, learning_rate=0.05,max_depth=7,n_jobs=-1)\n",
    "\n",
    "# Hyperparameter grid for XGBoost\n",
    "xgb_param_grid = {\n",
    "    'n_estimators': [7, 15],\n",
    "    'subsample': [0.8, 0.9],\n",
    "}\n",
    "\n",
    "# Perform GridSearchCV for XGBoost (same as before)\n",
    "roc_auc_scorer = make_scorer(roc_auc_score, greater_is_better=True)\n",
    "xgb_grid_search = GridSearchCV(estimator=xgb_model, param_grid=xgb_param_grid, cv=2, scoring=roc_auc_scorer, n_jobs=-1)\n",
    "xgb_grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Use the best XGBoost model from GridSearch\n",
    "best_xgb = xgb_grid_search.best_estimator_\n",
    "\n",
    "# # StackingClassifier with XGBoost and Neural Network\n",
    "# stacking_model = StackingClassifier(\n",
    "#     estimators=[('xgb', best_xgb), ('nn', nn_model)],\n",
    "#     final_estimator=LogisticRegression()\n",
    "# )\n",
    "#\n",
    "# # Fit the stacking model\n",
    "# stacking_model.fit(X_train, y_train)\n",
    "\n",
    "# Predict on the validation set\n",
    "# y_pred = stacking_model.predict(X_valid)\n",
    "# 1. 对神经网络进行预测\n",
    "nn_pred_train = nn_model.predict(X_train)  # 神经网络在训练集上的预测\n",
    "nn_pred_valid = nn_model.predict(X_valid)  # 神经网络在验证集上的预测\n",
    "nn_pred_test = nn_model.predict(df_test.drop(['probability'], axis=1))  # 神经网络在测试集上的预测\n",
    "\n",
    "# 2. 对XGBoost进行预测\n",
    "# xgb_pred_train = best_xgb.predict_proba(X_train)[:, 1]  # XGBoost在训练集上的预测概率\n",
    "xgb_pred_valid = best_xgb.predict_proba(X_valid)[:, 1]  # XGBoost在验证集上的预测概率\n",
    "xgb_pred_test = best_xgb.predict_proba(df_test.drop(['probability'], axis=1))[:, 1]  # XGBoost在测试集上的预测概率\n",
    "\n",
    "# 3. 对预测概率进行平均\n",
    "train_pred_avg = (nn_pred_train.flatten()*0.7 + xgb_pred_train*0.3)\n",
    "valid_pred_avg = (nn_pred_valid.flatten()*0.7 + xgb_pred_valid*0.3)\n",
    "test_pred_avg = (nn_pred_test.flatten()*0.7 + xgb_pred_test*0.3)\n",
    "\n",
    "# 4. 对验证集进行评估\n",
    "print(\"Validation ROC AUC Score (Average Prediction):\", roc_auc_score(y_valid, valid_pred_avg))\n",
    "\n",
    "# 5. 输出测试集的预测结果\n",
    "df_test['probability'] = test_pred_avg\n",
    "df_test[['user_id', 'merchant_id', 'probability']].to_csv('predictions.csv', index=False)\n",
    "\n",
    "# # Predict on the test set\n",
    "# df_test['probability'] = stacking_model.predict(df_test.drop(['probability'], axis=1))\n",
    "#\n",
    "# # Output the predictions to a CSV file\n",
    "# df_test[['user_id', 'merchant_id', 'probability']].to_csv('predictions.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
